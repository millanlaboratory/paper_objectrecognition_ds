
@article{khaliliardali_action_2015,
	title = {Action prediction based on anticipatory brain potentials during simulated driving},
	volume = {12},
	issn = {1741-2552},
	url = {http://stacks.iop.org/1741-2552/12/i=6/a=066006},
	doi = {10.1088/1741-2560/12/6/066006},
	abstract = {Objective. The ability of an automobile to infer the driver’s upcoming actions directly from neural signals could enrich the interaction of the car with its driver. Intelligent vehicles fitted with an on-board brain–computer interface able to decode the driver’s intentions can use this information to improve the driving experience. In this study we investigate the neural signatures of anticipation of specific actions, namely braking and accelerating. Approach. We investigated anticipatory slow cortical potentials in electroencephalogram recorded from 18 healthy participants in a driving simulator using a variant of the contingent negative variation (CNV) paradigm with Go and No-go conditions: count-down numbers followed by ‘Start’/‘Stop’ cue. We report decoding performance before the action onset using a quadratic discriminant analysis classifier based on temporal features. Main results. (i) Despite the visual and driving related cognitive distractions, we show the presence of anticipatory event related potentials locked to the stimuli onset similar to the widely reported CNV signal (with an average peak value of −8 μ V at electrode Cz). (ii) We demonstrate the discrimination between cases requiring to perform an action upon imperative subsequent stimulus ( Go condition, e.g. a ‘Red’ traffic light) versus events that do not require such action ( No-go condition; e.g. a ‘Yellow’ light); with an average single trial classification performance of 0.83 ± 0.13 for braking and 0.79 ± 0.12 for accelerating (area under the curve). (iii) We show that the centro-medial anticipatory potentials are observed as early as 320 ± 200 ms before the action with a detection rate of 0.77 ± 0.12 in offline analysis. Significance. We show for the first time the feasibility of predicting the driver’s intention through decoding anticipatory related potentials during simulated car driving with high recognition rates.},
	language = {en},
	number = {6},
	urldate = {2015-10-07},
	journal = {Journal of Neural Engineering},
	author = {Khaliliardali, Zahra and Chavarriaga, Ricardo and Gheorghe, Lucian Andrei and Millán, José del R.},
	year = {2015},
	pages = {066006},
	file = {IOP Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/WUCJKM5Q/Khaliliardali et al. - 2015 - Action prediction based on anticipatory brain pote.pdf:application/pdf}
}

@article{noauthor_evoked_2014,
	title = {Evoked {Neural} {Responses} to {Events} in {Video}},
	volume = {8},
	url = {http://adsabs.harvard.edu/abs/2014ISTSP...8..358.},
	doi = {10.1109/JSTSP.2014.2313022},
	abstract = {Not Available},
	urldate = {2016-04-12},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	month = jun,
	year = {2014},
	pages = {358--365}
}

@article{uscumlic_active_2016,
	title = {Active visual search in non-stationary scenes: coping with temporal variability and uncertainty},
	volume = {13},
	issn = {1741-2552},
	shorttitle = {Active visual search in non-stationary scenes},
	url = {http://stacks.iop.org/1741-2552/13/i=1/a=016015},
	doi = {10.1088/1741-2560/13/1/016015},
	abstract = {Objective . State-of-the-art experiments for studying neural processes underlying visual cognition often constrain sensory inputs (e.g., static images) and our behavior (e.g., fixed eye-gaze, long eye fixations), isolating or simplifying the interaction of neural processes. Motivated by the non-stationarity of our natural visual environment, we investigated the electroencephalography (EEG) correlates of visual recognition while participants overtly performed visual search in non-stationary scenes. We hypothesized that visual effects (such as those typically used in human–computer interfaces) may increase temporal uncertainty (with reference to fixation onset) of cognition-related EEG activity in an active search task and therefore require novel techniques for single-trial detection. Approach. We addressed fixation-related EEG activity in an active search task with respect to stimulus-appearance styles and dynamics. Alongside popping-up stimuli, our experimental study embraces two composite appearance styles based on fading-in, enlarging , and motion effects. Additionally, we explored whether the knowledge obtained in the pop-up experimental setting can be exploited to boost the EEG-based intention-decoding performance when facing transitional changes of visual content. Main results. The results confirmed our initial hypothesis that the dynamic of visual content can increase temporal uncertainty of the cognition-related EEG activity in active search with respect to fixation onset. This temporal uncertainty challenges the pivotal aim to keep the decoding performance constant irrespective of visual effects. Importantly, the proposed approach for EEG decoding based on knowledge transfer between the different experimental settings gave a promising performance. Significance. Our study demonstrates that the non-stationarity of visual scenes is an important factor in the evolution of cognitive processes, as well as in the dynamic of ocular behavior (i.e., dwell time and fixation duration) in an active search task. In addition, our method to improve single-trial detection performance in this adverse scenario is an important step in making brain–computer interfacing technology available for human–computer interaction applications.},
	language = {en},
	number = {1},
	urldate = {2016-04-12},
	journal = {Journal of Neural Engineering},
	author = {Ušćumlić, Marija and Blankertz, Benjamin},
	year = {2016},
	pages = {016015},
	file = {IOP Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/JDU4HDKN/Ušćumlić and Blankertz - 2016 - Active visual search in non-stationary scenes cop.pdf:application/pdf}
}

@article{jangraw_neurally_2014,
	title = {Neurally and ocularly informed graph-based models for searching 3D environments},
	volume = {11},
	issn = {1741-2552},
	url = {http://stacks.iop.org/1741-2552/11/i=4/a=046003},
	doi = {10.1088/1741-2560/11/4/046003},
	abstract = {Objective. As we move through an environment, we are constantly making assessments, judgments and decisions about the things we encounter. Some are acted upon immediately, but many more become mental notes or fleeting impressions—our implicit ‘labeling’ of the world. In this paper, we use physiological correlates of this labeling to construct a hybrid brain–computer interface (hBCI) system for efficient navigation of a 3D environment. Approach. First, we record electroencephalographic (EEG), saccadic and pupillary data from subjects as they move through a small part of a 3D virtual city under free-viewing conditions. Using machine learning, we integrate the neural and ocular signals evoked by the objects they encounter to infer which ones are of subjective interest to them. These inferred labels are propagated through a large computer vision graph of objects in the city, using semi-supervised learning to identify other, unseen objects that are visually similar to the labeled ones. Finally, the system plots an efficient route to help the subjects visit the ‘similar’ objects it identifies. Main results. We show that by exploiting the subjects’ implicit labeling to find objects of interest instead of exploring naively, the median search precision is increased from 25\% to 97\%, and the median subject need only travel 40\% of the distance to see 84\% of the objects of interest. We also find that the neural and ocular signals contribute in a complementary fashion to the classifiers’ inference of subjects’ implicit labeling. Significance. In summary, we show that neural and ocular signals reflecting subjective assessment of objects in a 3D environment can be used to inform a graph-based learning model of that environment, resulting in an hBCI system that improves navigation and information delivery specific to the user’s interests.},
	language = {en},
	number = {4},
	urldate = {2016-04-12},
	journal = {Journal of Neural Engineering},
	author = {Jangraw, David C. and Wang, Jun and Lance, Brent J. and Chang, Shih-Fu and Sajda, Paul},
	year = {2014},
	pages = {046003},
	file = {IOP Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/G58PCFZK/Jangraw et al. - 2014 - Neurally and ocularly informed graph-based models .pdf:application/pdf}
}

@article{oconnell_supramodal_2012,
	title = {A supramodal accumulation-to-bound signal that determines perceptual decisions in humans},
	volume = {15},
	copyright = {© 2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v15/n12/full/nn.3248.html},
	doi = {10.1038/nn.3248},
	abstract = {In theoretical accounts of perceptual decision-making, a decision variable integrates noisy sensory evidence and determines action through a boundary-crossing criterion. Signals bearing these very properties have been characterized in single neurons in monkeys, but have yet to be directly identified in humans. Using a gradual target detection task, we isolated a freely evolving decision variable signal in human subjects that exhibited every aspect of the dynamics observed in its single-neuron counterparts. This signal could be continuously tracked in parallel with fully dissociable sensory encoding and motor preparation signals, and could be systematically perturbed mid-flight during decision formation. Furthermore, we found that the signal was completely domain general: it exhibited the same decision-predictive dynamics regardless of sensory modality and stimulus features and tracked cumulative evidence even in the absence of overt action. These findings provide a uniquely clear view on the neural determinants of simple perceptual decisions in humans.},
	language = {en},
	number = {12},
	urldate = {2016-04-12},
	journal = {Nature Neuroscience},
	author = {O'Connell, Redmond G. and Dockree, Paul M. and Kelly, Simon P.},
	month = dec,
	year = {2012},
	keywords = {Decision, Neuronal physiology},
	pages = {1729--1735},
	file = {Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/6KCFJ3CF/O'Connell et al. - 2012 - A supramodal accumulation-to-bound signal that det.pdf:application/pdf;Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/AQZBK4M2/O'Connell et al. - 2012 - A supramodal accumulation-to-bound signal that det.pdf:application/pdf;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/QMVH844A/nn.3248.html:text/html;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/5FXMDG9U/nn.3248.html:text/html}
}

@article{graichen_sphara_2015,
	title = {{SPHARA} - {A} {Generalized} {Spatial} {Fourier} {Analysis} for {Multi}-{Sensor} {Systems} with {Non}-{Uniformly} {Arranged} {Sensors}: {Application} to {EEG}},
	volume = {10},
	issn = {1932-6203},
	shorttitle = {{SPHARA} - {A} {Generalized} {Spatial} {Fourier} {Analysis} for {Multi}-{Sensor} {Systems} with {Non}-{Uniformly} {Arranged} {Sensors}},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121741},
	doi = {10.1371/journal.pone.0121741},
	abstract = {Important requirements for the analysis of multichannel EEG data are efficient techniques for signal enhancement, signal decomposition, feature extraction, and dimensionality reduction. We propose a new approach for spatial harmonic analysis (SPHARA) that extends the classical spatial Fourier analysis to EEG sensors positioned non-uniformly on the surface of the head. The proposed method is based on the eigenanalysis of the discrete Laplace-Beltrami operator defined on a triangular mesh. We present several ways to discretize the continuous Laplace-Beltrami operator and compare the properties of the resulting basis functions computed using these discretization methods. We apply SPHARA to somatosensory evoked potential data from eleven volunteers and demonstrate the ability of the method for spatial data decomposition, dimensionality reduction and noise suppression. When employing SPHARA for dimensionality reduction, a significantly more compact representation can be achieved using the FEM approach, compared to the other discretization methods. Using FEM, to recover 95\% and 99\% of the total energy of the EEG data, on average only 35\% and 58\% of the coefficients are necessary. The capability of SPHARA for noise suppression is shown using artificial data. We conclude that SPHARA can be used for spatial harmonic analysis of multi-sensor data at arbitrary positions and can be utilized in a variety of other applications.},
	number = {4},
	urldate = {2016-06-27},
	journal = {PLOS ONE},
	author = {Graichen, Uwe and Eichardt, Roland and Fiedler, Patrique and Strohmeier, Daniel and Zanow, Frank and Haueisen, Jens},
	month = apr,
	year = {2015},
	keywords = {Eigenvalues, Eigenvectors, Manifolds, Signal filtering, White noise, Fourier Analysis, Principal Component Analysis, electroencephalography},
	pages = {e0121741},
	file = {Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/XJC74BPG/Graichen et al. - 2015 - SPHARA - A Generalized Spatial Fourier Analysis fo.pdf:application/pdf;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/Z66IZZAM/article.html:text/html}
}

@article{nikolaev_combining_2016,
	title = {Combining {EEG} and eye movement recording in free viewing: {Pitfalls} and possibilities},
	volume = {107},
	issn = {0278-2626},
	shorttitle = {Combining {EEG} and eye movement recording in free viewing},
	url = {http://www.sciencedirect.com/science/article/pii/S0278262616301051},
	doi = {10.1016/j.bandc.2016.06.004},
	abstract = {Co-registration of EEG and eye movement has promise for investigating perceptual processes in free viewing conditions, provided certain methodological challenges can be addressed. Most of these arise from the self-paced character of eye movements in free viewing conditions. Successive eye movements occur within short time intervals. Their evoked activity is likely to distort the EEG signal during fixation. Due to the non-uniform distribution of fixation durations, these distortions are systematic, survive across-trials averaging, and can become a source of confounding. We illustrate this problem with effects of sequential eye movements on the evoked potentials and time-frequency components of EEG and propose a solution based on matching of eye movement characteristics between experimental conditions. The proposal leads to a discussion of which eye movement characteristics are to be matched, depending on the EEG activity of interest. We also compare segmentation of EEG into saccade-related epochs relative to saccade and fixation onsets and discuss the problem of baseline selection and its solution. Further recommendations are given for implementing EEG-eye movement co-registration in free viewing conditions. By resolving some of the methodological problems involved, we aim to facilitate the transition from the traditional stimulus-response paradigm to the study of visual perception in more naturalistic conditions.},
	urldate = {2016-08-15},
	journal = {Brain and Cognition},
	author = {Nikolaev, Andrey R. and Meghanathan, Radha Nila and van Leeuwen, Cees},
	month = aug,
	year = {2016},
	keywords = {Eye tracking, Eye-fixation related potentials, Peri-saccadic brain activity, Saccade-related EEG distortion, Unconstrained visual exploration, electroencephalography},
	pages = {55--83},
	file = {ScienceDirect Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/WW9QMASB/Nikolaev et al. - 2016 - Combining EEG and eye movement recording in free v.pdf:application/pdf;ScienceDirect Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/M68RKWVT/S0278262616301051.html:text/html}
}

@article{kamienkowski_fixation-related_2012,
	title = {Fixation-related potentials in visual search: {A} combined {EEG} and eye tracking study},
	volume = {12},
	issn = {1534-7362},
	shorttitle = {Fixation-related potentials in visual search},
	url = {http://www.journalofvision.org/lookup/doi/10.1167/12.7.4},
	doi = {10.1167/12.7.4},
	language = {en},
	number = {7},
	urldate = {2016-10-09},
	journal = {Journal of Vision},
	author = {Kamienkowski, J. E. and Ison, M. J. and Quiroga, R. Q. and Sigman, M.},
	month = jul,
	year = {2012},
	pages = {4--4}
}

@article{zhang_eeg-based_2015,
	title = {{EEG}-based decoding of error-related brain activity in a real-world driving task},
	volume = {12},
	issn = {1741-2552},
	url = {http://stacks.iop.org/1741-2552/12/i=6/a=066028},
	doi = {10.1088/1741-2560/12/6/066028},
	abstract = {Objectives. Recent studies have started to explore the implementation of brain–computer interfaces (BCI) as part of driving assistant systems. The current study presents an EEG-based BCI that decodes error-related brain activity. Such information can be used, e.g., to predict driver’s intended turning direction before reaching road intersections. Approach. We executed experiments in a car simulator ( N = 22) and a real car ( N = 8). While subject was driving, a directional cue was shown before reaching an intersection, and we classified the presence or not of an error-related potentials from EEG to infer whether the cued direction coincided with the subject’s intention. In this protocol, the directional cue can correspond to an estimation of the driving direction provided by a driving assistance system. We analyzed ERPs elicited during normal driving and evaluated the classification performance in both offline and online tests. Results. An average classification accuracy of 0.698 ± 0.065 was obtained in offline experiments in the car simulator, while tests in the real car yielded a performance of 0.682 ± 0.059. The results were significantly higher than chance level for all cases. Online experiments led to equivalent performances in both simulated and real car driving experiments. These results support the feasibility of decoding these signals to help estimating whether the driver’s intention coincides with the advice provided by the driving assistant in a real car. Significance. The study demonstrates a BCI system in real-world driving, extending the work from previous simulated studies. As far as we know, this is the first online study in real car decoding driver’s error-related brain activity. Given the encouraging results, the paradigm could be further improved by using more sophisticated machine learning approaches and possibly be combined with applications in intelligent vehicles.},
	language = {en},
	number = {6},
	urldate = {2016-10-11},
	journal = {Journal of Neural Engineering},
	author = {Zhang, H. and Chavarriaga, R. and Khaliliardali, Z. and Gheorghe, L. and Iturrate, I. and Millán, J. d R.},
	year = {2015},
	pages = {066028},
	file = {IOP Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/8X65VAX4/Zhang et al. - 2015 - EEG-based decoding of error-related brain activity.pdf:application/pdf}
}

@inproceedings{renold_eeg_2014,
	title = {{EEG} correlates of active visual search during simulated driving: {An} exploratory study},
	shorttitle = {{EEG} correlates of active visual search during simulated driving},
	url = {https://infoscience.epfl.ch/record/200116},
	urldate = {2016-10-12},
	booktitle = {2014 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}},
	author = {Renold, Hadrian and Chavarriaga, Ricardo and Gheorghe, Lucian Andrei and Millán, José del R.},
	year = {2014},
	file = {Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/FQ9KCNG9/Renold et al. - 2014 - EEG correlates of active visual search during simu.pdf:application/pdf;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/QU3HANGU/200116.html:text/html}
}

@article{brouwer_distinguishing_2013,
	title = {Distinguishing between target and nontarget fixations in a visual search task using fixation-related potentials},
	volume = {13},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?articleid=2194100},
	doi = {10.1167/13.3.17},
	number = {3},
	urldate = {2017-02-27},
	journal = {Journal of Vision},
	author = {Brouwer, Anne-Marie and Reuderink, Boris and Vincent, Joris and Gerven, Marcel A. J. van and Erp, Jan B. F. van},
	month = sep,
	year = {2013},
	pages = {17--17},
	file = {Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/RWSDV22Z/Brouwer et al. - 2013 - Distinguishing between target and nontarget fixati.pdf:application/pdf;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/7TVNG4MZ/article.html:text/html}
}

@article{khaliliardali_detection_2012,
	title = {Detection of anticipatory brain potentials during car driving},
	volume = {2012},
	url = {http://dx.doi.org/10.1109/EMBC.2012.6346802},
	doi = {10.1109/EMBC.2012.6346802},
	abstract = {Recognition of driver's intention from electroencephalogram (EEG) can be helpful in developing an in-car brain computer interface (BCI) systems for intelligent cars. This could be beneficial in enhancing the quality of interaction between the driver and the car to provide the response of the intelligent cars in line with driver's intention. We proposed investigating anticipation as the cognitive state leading to specific actions during car driving. An experimental protocol is designed for recording EEG from 6 subjects while driving the virtual reality driving simulator. The experimental protocol is a variant of the contingent negative variation (CNV) paradigm with Go and No-go conditions in driving framework. The results presented in this study support the presence of the slow cortical anticipatory potentials in EEG grand averages and also confirm the discriminability of these potentials in offline single trial classification with the average of 0.76 ± 0.12 in area under the curve (AUC).},
	journal = {Conf Proc IEEE Eng Med Biol Soc},
	author = {Khaliliardali, Zahra and Chavarriaga, Ricardo and Gheorghe, Lucian Andrei and Millán, José del R},
	year = {2012},
	pmid = {23366763},
	keywords = {Action Potentials, Adult, Automobile Driving, Brain, Electroencephalography, Evoked Potentials, Female, Humans, Male, physiology, Time Factors, Young Adult},
	pages = {3829--3832}
}

@article{khaliliardali_bci_2016,
	title = {{BCI} based on {Neural} {Correlates} of {Anticipatory} {Behavior} during {Driving}},
	url = {https://infoscience.epfl.ch/record/218037},
	doi = {10.5075/epfl-thesis-6910, urn:nbn:ch:bel-epfl-thesis6910-8},
	urldate = {2017-03-02},
	author = {Khaliliardali, Zahra},
	year = {2016},
	file = {Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/J2H3BG5C/218037.html:text/html}
}

@inproceedings{gheorghe_steering_2013,
	title = {Steering timing prediction in a driving simulator task},
	doi = {10.1109/EMBC.2013.6611147},
	abstract = {In this paper we present the preliminary results of a pioneering attempt to predict the timing of steering actions in a driving task from non-invasive EEG measurements. The experiment took place with the subjects driving a car at a constant speed on a simulated highway in a driving simulator. The EEG activity was analyzed during periods of straight driving and during lane change actions. Classifiers were built on the signals recorded over the motor areas for straight and pre-steering periods. The onset of the steering actions was detected on average 811ms before the action with a 74.6\% true positive rate.},
	booktitle = {2013 35th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Gheorghe, L. and Chavarriaga, R. and Millán, J. d R.},
	month = jul,
	year = {2013},
	keywords = {driving simulator task, electroencephalogram, electroencephalography, Electroencephalography, feature extraction, lane change actions, medical signal processing, motor areas, noninvasive EEG measurements, Road transportation, signal classification, signal classifiers, steering timing prediction, Training, Trajectory, Vehicles, Visualization},
	pages = {6913--6916},
	file = {IEEE Xplore Abstract Record:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/UMUAGXDG/6611147.html:text/html;IEEE Xplore Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/2U8GSLKC/Gheorghe et al. - 2013 - Steering timing prediction in a driving simulator .pdf:application/pdf}
}

@inproceedings{devillez_p300_2015,
	title = {The {P}300 potential for fixations onto target object when exploring natural scenes during a visual task after denoising overlapped {EFRP}},
	doi = {10.1109/NER.2015.7146801},
	abstract = {Electroencephalography (EEG) studies have largely reported the P300 Event Related Potential (ERP) elicited by target stimulus processing compared to non-target stimulus. These studies used constrain experimental paradigms during which participants did not move their eyes. However, during more ecological paradigms where participants can move their eyes, the P300 potential might be more difficult to identify due to the overlapped potentials elicited by consecutive ocular fixations. In this study, we use the xDawn algorithm for denoising the overlapped Eye-Fixation Related Potentials (EFRP), and we observe the P300 potential elicited on the first, but also on the consecutive subsequent fixations landed on the target stimuli.},
	booktitle = {2015 7th {International} {IEEE}/{EMBS} {Conference} on {Neural} {Engineering} ({NER})},
	author = {Devillez, H. and Kristensen, E. and Guyader, N. and Rivet, B. and Guérin-Dugué, A.},
	month = apr,
	year = {2015},
	keywords = {bioelectric potentials, Conferences, Context, ecological paradigms, Electric potential, electroencephalography, Electroencephalography, eye, eye movement, medical signal processing, Neural engineering, Noise reduction, overlapped EFRP denoising, overlapped eye-fixation related potential denoising, P300 event related potential, signal denoising, target stimulus processing, Visualization, xDawn algorithm},
	pages = {1024--1027}
}

@inproceedings{zhang_inferring_2013,
	title = {Inferring driver's turning direction through detection of error related brain activity},
	doi = {10.1109/EMBC.2013.6609971},
	abstract = {This work presents EEG-based Brain-computer interface (BCI) that uses error related brain activity to improve the prediction of driver's intended turning direction. In experiments while subjects drive in a realistic car simulator, we show a directional cue before reaching intersection, and analyze error related EEG potential to infer if the presented direction coincides with the driver's intention. In this protocol, the directional cue provides an initial estimation of the driving direction (based on EEG, environmental or previous driving habits), and we focus on the recognition of error-potentials it may elicit. Experiments with 7 healthy human subjects yield an average classification 0.69±0.16, which confirms the feasibility of decoding these signals to help estimating driver's turning direction. This study can be further exploited by intelligent cars to tune their driving assistant systems to improve their performance and enhance the driving experience.},
	booktitle = {2013 35th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Zhang, Huaijian and Chavarriaga, Ricardo and Gheorghe, Lucian and Millán, José del R.},
	month = jul,
	year = {2013},
	note = {ISSN: 1094-687X, 1558-4615},
	keywords = {Accuracy, Adult, Automobile Driving, automobiles, brain, Brain, brain-computer interfaces, Brain-Computer Interfaces, driver intended turning direction, EEG-based BCI, EEG-based brain-computer interface, Electrodes, electroencephalography, Electroencephalography, encoding, error related brain activity, Evoked Potentials, Female, Humans, intelligent cars, Male, medical signal detection, medical signal processing, ROC Curve, signal classification, signal decoding, Turning, Vehicles, Visualization},
	pages = {2196--2199},
	file = {IEEE Xplore Abstract Record:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/ZRTPQMP7/6609971.html:text/html;IEEE Xplore Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/7AGSIQJF/Zhang et al. - 2013 - Inferring driver's turning direction through detec.pdf:application/pdf}
}

@article{rezeika_braincomputer_2018,
	title = {Brain–{Computer} {Interface} {Spellers}: {A} {Review}},
	volume = {8},
	issn = {2076-3425},
	shorttitle = {Brain–{Computer} {Interface} {Spellers}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5924393/},
	doi = {10.3390/brainsci8040057},
	abstract = {A Brain–Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. A BCI-speller can be considered as one of the first published BCI applications and has opened the gate for many advances in the field. Although many BCI-spellers have been developed during the last few decades, to our knowledge, no reviews have described the different spellers proposed and studied in this vital field. The presented speller systems are categorized according to major BCI paradigms: P300, steady-state visual evoked potential (SSVEP), and motor imagery (MI). Different BCI paradigms require specific electroencephalogram (EEG) signal features and lead to the development of appropriate Graphical User Interfaces (GUIs). The purpose of this review is to consolidate the most successful BCI-spellers published since 2010, while mentioning some other older systems which were built explicitly for spelling purposes. We aim to assist researchers and concerned individuals in the field by illustrating the highlights of different spellers and presenting them in one review. It is almost impossible to carry out an objective comparison between different spellers, as each has its variables, parameters, and conditions. However, the gathered information and the provided taxonomy about different BCI-spellers can be helpful, as it could identify suitable systems for first-hand users, as well as opportunities of development and learning from previous studies for BCI researchers.},
	number = {4},
	urldate = {2019-11-11},
	journal = {Brain Sciences},
	author = {Rezeika, Aya and Benda, Mihaly and Stawicki, Piotr and Gembler, Felix and Saboor, Abdul and Volosyak, Ivan},
	month = mar,
	year = {2018},
	pmid = {29601538},
	pmcid = {PMC5924393},
	file = {PubMed Central Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/882EP5Q5/Rezeika et al. - 2018 - Brain–Computer Interface Spellers A Review.pdf:application/pdf}
}

@article{sellers_brain-computer_2010,
	title = {A brain-computer interface for long-term independent home use},
	volume = {11},
	issn = {1748-2968},
	url = {https://doi.org/10.3109/17482961003777470},
	doi = {10.3109/17482961003777470},
	abstract = {Our objective was to develop and validate a new brain-computer interface (BCI) system suitable for long-term independent home use by people with severe motor disabilities. The BCI was used by a 51-year-old male with ALS who could no longer use conventional assistive devices. Caregivers learned to place the electrode cap, add electrode gel, and turn on the BCI. After calibration, the system allowed the user to communicate via EEG. Re-calibration was performed remotely (via the internet), and BCI accuracy assessed in periodic tests. Reports of BCI usefulness by the user and the family were also recorded. Results showed that BCI accuracy remained at 83\% (r = −.07, n.s.) for over 2.5 years (1.4\% expected by chance). The BCI user and his family state that the BCI had restored his independence in social interactions and at work. He uses the BCI to run his NIH-funded research laboratory and to communicate via e-mail with family, friends, and colleagues. In addition to this first user, several other similarly disabled people are now using the BCI in their daily lives. In conclusion, long-term independent home use of this BCI system is practical for severely disabled people, and can contribute significantly to quality of life and productivity.},
	number = {5},
	urldate = {2019-11-11},
	journal = {Amyotrophic Lateral Sclerosis},
	author = {Sellers, Eric W. and Vaughan, Theresa M. and Wolpaw, Jonathan R.},
	month = oct,
	year = {2010},
	pmid = {20583947},
	keywords = {brain-computer interface, electroencephalogram, Amyotrophic lateral sclerosis, assistive communication, P300 event-related potential},
	pages = {449--455},
	file = {Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/3TK94LPP/17482961003777470.html:text/html}
}

@article{holz_long-term_2015,
	series = {The {Fifth} {International} {Brain}-{Computer} {Interface} {Meeting} {Presents} {Clinical} and {Translational} {Developments} in {Brain}-{Computer} {Interface} {Research}},
	title = {Long-{Term} {Independent} {Brain}-{Computer} {Interface} {Home} {Use} {Improves} {Quality} of {Life} of a {Patient} in the {Locked}-{In} {State}: {A} {Case} {Study}},
	volume = {96},
	issn = {0003-9993},
	shorttitle = {Long-{Term} {Independent} {Brain}-{Computer} {Interface} {Home} {Use} {Improves} {Quality} of {Life} of a {Patient} in the {Locked}-{In} {State}},
	url = {http://www.sciencedirect.com/science/article/pii/S0003999314003657},
	doi = {10.1016/j.apmr.2014.03.035},
	abstract = {Objective
Despite intense brain-computer interface (BCI) research for {\textgreater}2 decades, BCIs have hardly been established at patients' homes. The current study aimed at demonstrating expert independent BCI home use by a patient in the locked-in state and the effect it has on quality of life.
Design
In this case study, the P300 BCI-controlled application Brain Painting was facilitated and installed at the patient's home. Family and caregivers were trained in setting up the BCI system. After every BCI session, the end user indicated subjective level of control, loss of control, level of exhaustion, satisfaction, frustration, and enjoyment. To monitor BCI home use, evaluation data of every session were automatically sent and stored on a remote server. Satisfaction with the BCI as an assistive device and subjective workload was indicated by the patient. In accordance with the user-centered design, usability of the BCI was evaluated in terms of its effectiveness, efficiency, and satisfaction. The influence of the BCI on quality of life of the end user was assessed.
Setting
At the patient's home.
Participant
A 73-year-old patient with amyotrophic lateral sclerosis in the locked-in state.
Interventions
Not applicable.
Main Outcome Measure
The BCI has been used by the patient independent of experts for {\textgreater}14 months. The patient painted in about 200 BCI sessions (1–3 times per week) with a mean painting duration of 81.86 minutes (SD=52.15, maximum: 230.41). BCI improved quality of life of the patient.
Results
In most of the BCI sessions the end user's satisfaction was high (mean=7.4, SD=3.24; range, 0–10). Dissatisfaction occurred mostly because of technical problems at the beginning of the study or varying BCI control. The subjective workload was moderate (mean=40.61; range, 0–100). The end user was highy satisfied with all components of the BCI (mean 4.42–5.0; range, 1–5). A perfect match between the user and the BCI technology was achieved (mean: 4.8; range, 1–5). Brain Painting had a positive impact on the patient's life on all three dimensions: competence (1.5), adaptability (2.17) and self-esteem (1.5); (range: –3 = maximum negative impact; 3 maximum positive impact). The patient had her first public art exhibition in July 2013; future exhibitions are in preparation.
Conclusions
Independent BCI home use is possible with high satisfaction for the end user. The BCI indeed positively influenced quality of life of the patient and supports social inclusion. Results demonstrate that visual P300 BCIs can be valuable for patients in the locked-in state even if other means of communication are still available (eye tracker).},
	language = {en},
	number = {3, Supplement},
	urldate = {2019-11-11},
	journal = {Archives of Physical Medicine and Rehabilitation},
	author = {Holz, Elisa Mira and Botrel, Loic and Kaufmann, Tobias and Kübler, Andrea},
	month = mar,
	year = {2015},
	keywords = {Rehabilitation, Brain-computer interfaces, Amyotrophic lateral sclerosis, Locked-in syndrome, Quality of life},
	pages = {S16--S26},
	file = {ScienceDirect Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/VTY3Z2KL/S0003999314003657.html:text/html}
}

@article{carabalona_light_2012,
	title = {Light on! {Real} world evaluation of a {P}300-based brain–computer interface ({BCI}) for environment control in a smart home},
	volume = {55},
	issn = {0014-0139},
	url = {https://doi.org/10.1080/00140139.2012.661083},
	doi = {10.1080/00140139.2012.661083},
	abstract = {Brain–computer interface (BCI) systems aim to enable interaction with other people and the environment without muscular activation by the exploitation of changes in brain signals due to the execution of cognitive tasks. In this context, the visual P300 potential appears suited to control smart homes through BCI spellers. The aim of this work is to evaluate whether the widely used character-speller is more sustainable than an icon-based one, designed to operate smart home environment or to communicate moods and needs. Nine subjects with neurodegenerative diseases and no BCI experience used both speller types in a real smart home environment. User experience during BCI tasks was evaluated recording concurrent physiological signals. Usability was assessed for each speller type immediately after use. Classification accuracy was lower for the icon-speller, which was also more attention demanding. However, in subjective evaluations, the effect of a real feedback partially counterbalanced the difficulty in BCI use. Practitioner Summary: Since inclusive BCIs require to consider interface sustainability, we evaluated different ergonomic aspects of the interaction of disabled users with a character-speller (goal: word spelling) and an icon-speller (goal: operating a real smart home). We found the first one as more sustainable in terms of accuracy and cognitive effort.},
	number = {5},
	urldate = {2019-11-11},
	journal = {Ergonomics},
	author = {Carabalona, Roberta and Grossi, Ferdinando and Tessadri, Adam and Castiglioni, Paolo and Caracciolo, Antonio and Munari, Ilaria de},
	month = may,
	year = {2012},
	pmid = {22455346},
	keywords = {physiology, advanced human–machine interfaces, controls and input devices, mental fatigue, user needs analysis},
	pages = {552--563},
	file = {Full Text PDF:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/YT6F5JRS/Carabalona et al. - 2012 - Light on! Real world evaluation of a P300-based br.pdf:application/pdf;Snapshot:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/I7Y2WQRN/00140139.2012.html:text/html}
}

@misc{noauthor_about_nodate,
	title = {About - {VDrift}},
	url = {http://vdrift.net/},
	urldate = {2019-11-14},
	file = {About - VDrift:/home/ruslan/.zotero/zotero/u3gbzawz.default/zotero/storage/FAMK2HTE/vdrift.net.html:text/html}
}

@article{rosenthal_evoked_2014,
	title = {Evoked {Neural} {Responses} to {Events} in {Video}},
	volume = {8},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/6777281/},
	doi = {10.1109/JSTSP.2014.2313022},
	abstract = {In contrast to static imagery, detection of events of interest in video involves evidence accumulation across space and time; the observer is required to integrate features from both motion and form to decide whether a behavior constituents a target event. Do such events that extend in time elicit evoked responses of similar strength as evoked responses associated with instantaneous events such as the presentation of a static target image? Using a set of simulated scenarios, with avatars/actors having different behaviors, we identiﬁed evoked neural activity discriminative of target vs. distractor events (behaviors) at discrimination levels that are comparable to static imagery. EEG discriminative activity was largely in the time-locked evoked response and not in oscillatory activity, with the exception of very low EEG frequency bands such as delta and theta, which simply represent bands dominating the event related potential (ERP). The discriminative evoked response activity we see is observed in all target/distractor conditions and is robust across different recordings from the same subjects. The results suggest that we have identiﬁed a robust neural correlate of target detection in video, at least in terms of the stimulus set we used—i.e., dynamic behavior of an individual in a low clutter environment. Additional work is needed to test a larger variety of behaviors and more diverse environments.},
	language = {en},
	number = {3},
	urldate = {2019-11-14},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Rosenthal, Daniel and DeGuzman, Paul and Parra, Lucas C. and Sajda, Paul},
	month = jun,
	year = {2014},
	pages = {358--365},
	file = {evoked_p_video.pdf:/home/ruslan/Documents/articles/attention/evoked_p_video.pdf:application/pdf}
}